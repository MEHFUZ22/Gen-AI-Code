# -*- coding: utf-8 -*-
"""Transfomers_week3_01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wg3SVEcT0_F6OmPHo4CXkfz9lu1RDYw2

#Sentiment Analysis
"""

from transformers import pipeline


classifier=pipeline("sentiment-analysis")

review=[
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]


result=classifier(review)
print(result)



from transformers import pipeline


classifier=pipeline(task="sentiment-analysis")

review="The film was kind of good but not very good",




result=classifier(review)
print(result)

from transformers import pipeline


classifier=pipeline(task="sentiment-analysis")

review="I am good not that good",




result=classifier(review)
print(result)

from transformers import pipeline


classifier=pipeline("sentiment-analysis")

reviews=[
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]


results=classifier(reviews)
print(results)

for review ,result in zip(reviews,results):
  print(f" Review : {review} | {result['label']} | {result['score']}")

"""#Summarisation




"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("t5-small")
output_text = summary[0]['summary_text']
print(output_text)
print("Token count:", len(tokenizer.encode(output_text)))

text = """Artificial Intelligence (AI) has rapidly become an integral part of our daily
lives. From voice assistants like Siri and Alexa to recommendation systems on
platforms like Netflix and Amazon, AI is shaping how we interact with technology.
It is no longer confined to the realm of science fiction but is now a practical tool
that enhances efficiency and convenience.
One of the key drivers of AI adoption is its ability to analyze vast amounts of data
and make predictions or decisions faster than humans. In industries like
healthcare, AI-powered systems are used to detect diseases such as cancer in
their early stages, saving countless lives. Similarly, in the financial sector, AI
algorithms help identify fraudulent transactions and streamline investment
strategies.
However, the rise of AI also raises ethical concerns. Issues such as data privacy,
algorithmic bias, and job displacement are frequently debated. While AI can
improve productivity, it also has the potential to replace human jobs, particularly
in repetitive or manual tasks. This highlights the need for responsible AI
development and robust regulations to ensure that technology benefits everyone.
Despite these challenges, the potential of AI is immense. Researchers are
constantly working on advancements in natural language processing, computer
vision, and machine learning to make AI even more intelligent and adaptable. As
technology continues to evolve, it is likely that AI will play an even greater role in
shaping our future."""

from transformers import pipeline

summarizer=pipeline("summarization")
summary=summarizer(text)
print(summary)

print(summary[0]['summary_text'])

text = """Artificial Intelligence (AI) has rapidly become an integral part of our daily
lives. From voice assistants like Siri and Alexa to recommendation systems on
platforms like Netflix and Amazon, AI is shaping how we interact with technology.
It is no longer confined to the realm of science fiction but is now a practical tool
that enhances efficiency and convenience.
One of the key drivers of AI adoption is its ability to analyze vast amounts of data
and make predictions or decisions faster than humans. In industries like
healthcare, AI-powered systems are used to detect diseases such as cancer in
their early stages, saving countless lives. Similarly, in the financial sector, AI
algorithms help identify fraudulent transactions and streamline investment
strategies.
However, the rise of AI also raises ethical concerns. Issues such as data privacy,
algorithmic bias, and job displacement are frequently debated. While AI can
improve productivity, it also has the potential to replace human jobs, particularly
in repetitive or manual tasks. This highlights the need for responsible AI
development and robust regulations to ensure that technology benefits everyone.
Despite these challenges, the potential of AI is immense. Researchers are
constantly working on advancements in natural language processing, computer
vision, and machine learning to make AI even more intelligent and adaptable. As
technology continues to evolve, it is likely that AI will play an even greater role in
shaping our future."""

from transformers import pipeline

summarizer=pipeline(task="summarization",model="t5-small",min_length=10,max_length=20)
summary=summarizer(text)
print(summary)

from transformers import pipeline

summarizer=pipeline(task="summarization",model="t5-small",min_length=5,max_length=5)
summary=summarizer(text)
print(summary)

"""to know the tokenizer count or lenght of output"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("t5-small")
output_text = summary[0]['summary_text']
print(output_text)
print("Token count:", len(tokenizer.encode(output_text)))

from transformers import pipeline

summarizer = pipeline(task="summarization", model="facebook/bart-large-cnn")
summary = summarizer(text, min_length=30, max_length=130)
print(summary[0]['summary_text'])

from transformers import pipeline

summarizer = pipeline(task="summarization", model="facebook/bart-large-cnn")
summary = summarizer(text, min_length=10, max_length=10)
print(summary[0]['summary_text'])

from transformers import AutoModelForSeq2SeqLM

model = AutoModelForSeq2SeqLM.from_pretrained("facebook/bart-large-cnn")

# Count total parameters
total_params = sum(p.numel() for p in model.parameters())
print(f"Total parameters: {total_params:,}")

"""#Questions and Answer"""

from transformers import pipeline


question_answering=pipeline(task="question-answering")

context="""
Name: Mehfuz Ur Rahman
Role: Big Data Developer
Phone: +91 9886746347
Email: mehfuz0831@gmail.com
LinkedIn: linkedin.com/in/mehfuzurrahman
Location: Bangalore, India

SUMMARY:
Skilled Big Data Engineer with 3 years of proven experience in designing and implementing efficient ETL processes. Proficient in Python, SQL, PySpark, and cloud-based data platforms. Strong collaborator with cross-functional teams to deliver high-quality solutions.

SKILLS:
- Programming: Python, SQL, PySpark
- Data & Technologies: Apache Spark, Databricks, Azure Cloud, Azure Data Lake, Azure Data Factory, Azure Synapse, Hive
- Data Engineering: ETL, Data Modelling, Data Warehousing
- Tools: Git, GitHub, CI/CD
- Cloud: AWS Redshift, AWS S3
- Others: Machine Learning, Data Structures

EXPERIENCE:

**Data Engineer**
Tata Consultancy Services
Bangalore | Oct 2021 – Sep 2024

Key Responsibilities:
- Designed and implemented data pipelines using Azure Databricks and ADF, reducing data processing time by 25%.
- Applied data quality checks using SQL and PySpark, improving accuracy by 15%.
- Tuned SQL queries to optimize data retrieval by 20%.
- Enhanced logic in PySpark and SQL to handle 60% of inconsistent data, improving quality.
- Automated the MDM outbound process for seamless data integration.
- Collaborated with teams and clients, documented processes to ensure knowledge transfer.

KEY ACHIEVEMENTS:
- **TCS Higher Talent Award**: Received “Elevate Wings” badge out of 200,000 participants for exceptional performance and impact.
- **TFactor Excellence**: Achieved a T-factor score of 2.99 (benchmark: 1.99), recognized for steep learning curve and high contributions.

CERTIFICATIONS:
1. Microsoft Certified Azure Data Engineer
2. Masters Program in Data Science (Simplilearn + IBM)
3. HackerRank – SQL Advanced
4. Google Data Analytics Professional Certificate

EDUCATION:
Bachelor of Engineering – Electronics and Communication
Visvesvaraya Technological University, Bangalore
Aug 2017 – Aug 2021

LANGUAGES:
- English
- Kannada
- Hindi
- Bengali
"""

"""#"""

qs="tell me the company name in which I have worked?"
result=question_answering(question=qs,context=context)
print(result)

from transformers import pipeline

qa = pipeline(task="question-answering", model="deepset/roberta-base-squad2")
qs="tell me the company name in which I have worked?"

result = qa(question=qs, context=context)
print(result['answer'])

context="""
Mehfuz Ur Rahman is a Big Data Developer based in Bangalore, India, with 3 years of experience specializing in designing and implementing efficient ETL processes. He is proficient in Python, SQL, PySpark, and various cloud platforms including Azure Databricks, Azure Data Factory, Azure Synapse, Azure Data Lake, as well as AWS services like Redshift and S3. His skills encompass Apache Spark, data modeling, data warehousing, CI/CD, and machine learning fundamentals.

From October 2021 to September 2024, Mehfuz worked as a Data Engineer at Tata Consultancy Services, where he led the development of data pipelines that reduced processing time by 25%. He improved data accuracy by 15% through quality checks and optimized SQL queries to enhance data retrieval by 20%. Mehfuz also enhanced PySpark and SQL logic to handle inconsistent data efficiently and automated master data management outbound processes. He collaborated extensively with cross-functional teams and ensured thorough documentation for smooth knowledge transfer.

Mehfuz received notable recognition including the TCS Higher Talent Award among 200,000 participants and achieved a T-factor score of 2.99, surpassing the high proficiency benchmark. His certifications include Microsoft Certified Azure Data Engineer, a Masters Program in Data Science from Simplilearn in collaboration with IBM, HackerRank SQL Advanced, and Google Data Analytics Professional Certificate.

He holds a Bachelor of Engineering degree in Electronics and Communication from Visvesvaraya Technological University, Bangalore. Mehfuz is fluent in English, Kannada, Hindi, and Bengali.
"""

from transformers import pipeline

qa = pipeline(task="question-answering", model="deepset/roberta-base-squad2")
qs="total experience I have?"
result = qa(question=qs, context=context)
print(result['answer'])

from transformers import pipeline

qa = pipeline(task="question-answering", model="deepset/roberta-base-squad2")

def get_answer(question, context=None):
    if context is None:
        context = """
        Mehfuz Ur Rahman is a Big Data Developer based in Bangalore, India, with 3 years of experience specializing in designing and implementing efficient ETL processes. He is proficient in Python, SQL, PySpark, and various cloud platforms including Azure Databricks, Azure Data Factory, Azure Synapse, Azure Data Lake, as well as AWS services like Redshift and S3. His skills encompass Apache Spark, data modeling, data warehousing, CI/CD, and machine learning fundamentals.

        From October 2021 to September 2024, Mehfuz worked as a Data Engineer at Tata Consultancy Services, where he led the development of data pipelines that reduced processing time by 25%. He improved data accuracy by 15% through quality checks and optimized SQL queries to enhance data retrieval by 20%. Mehfuz also enhanced PySpark and SQL logic to handle inconsistent data efficiently and automated master data management outbound processes. He collaborated extensively with cross-functional teams and ensured thorough documentation for smooth knowledge transfer.

        Mehfuz received notable recognition including the TCS Higher Talent Award among 200,000 participants and achieved a T-factor score of 2.99, surpassing the high proficiency benchmark. His certifications include Microsoft Certified Azure Data Engineer, a Masters Program in Data Science from Simplilearn in collaboration with IBM, HackerRank SQL Advanced, and Google Data Analytics Professional Certificate.

        He holds a Bachelor of Engineering degree in Electronics and Communication from Visvesvaraya Technological University, Bangalore. Mehfuz is fluent in English, Kannada, Hindi, and Bengali.
        """
    result = qa(question=question, context=context)
    return result['answer']

# Example:
print(get_answer("What are Mehfuz's achievements?"))
print(get_answer("How many years of experience does Mehfuz have?"))

get_answer("tell me the period mehfuz worked in in tata consultancy service")

"""#Translation"""

from transformers import pipeline


translator=pipeline(task="translation_en_to_fr",model='t5-small')

text='mehfuz is good guy'
result=translator(text)
print(result)

from transformers import pipeline


translator=pipeline(task="translation_en_to_fr",model='t5-base')

text='mehfuz is good guy'
result=translator(text)
print(result)

!cat /proc/meminfo | grep Mem
!df -h  # check disk space

"""##Entities"""

from transformers import pipeline



ner=pipeline(task='ner')

text='Elon Musk founded SpaceX in 2002 and Tesla Motors in 2003'
entities=ner(text)
print(entities)

from transformers import pipeline



ner=pipeline(task='ner',grouped_entities=True)

text='Elon Musk founded SpaceX in 2002 and Tesla Motors in 2003'
entities=ner(text)
print(entities)

from transformers import pipeline



ner=pipeline(task='ner',grouped_entities=True)

text='Elon Musk founded SpaceX in 2002 and Tesla Motors in 2003'
entities=ner(text)
print(entities)



"""#Mask"""

from transformers import pipeline

fill_mask=pipeline(task="fill-mask",model="bert-base-uncased")
text="[MASK] is The capital of India "
predictions=fill_mask(text)
print(predictions)

from transformers import pipeline

fill_mask=pipeline(task="fill-mask",model="bert-base-uncased")
text="Capital of America is [MASK] "
predictions=fill_mask(text)
print(predictions)

from transformers import pipeline

fill_mask=pipeline(task="fill-mask")
text="Capital of America is <mask>"
predictions=fill_mask(text)
print(predictions)

"""#Text Generation Code"""

from transformers import pipeline

generator=pipeline(task="text-generation")

prompt="once upon a time there was fox"

generated_text=generator(prompt,max_length=50,num_return_sequences=2)
print(generated_text)

"""#Feature Extraction"""

from transformers import pipeline



feature_extractor=pipeline(task="feature-extraction",model="bert-base-uncased")

text="Mehfuz is very curious guy"
feature=feature_extractor(text,return_tensors="pt") #pt->pytorch | tensor

print(feature.shape)
print(feature)

from transformers import AutoTokenizer, AutoModel
import torch

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")

text = [
    "Mehfuz is very curious guy.",
    "He loves learning new things every day."
]

inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
outputs = model(**inputs)

# Get last hidden states (features)
features = outputs.last_hidden_state
print(features.shape)  # (batch_size, seq_len, hidden_size)
print(features)

"""#Zero Shot Classification"""

from transformers import pipeline


classifier=pipeline("zero-shot-classification",model="facebook/bart-large-mnli")

text="I have leno laptop which is very old now even the performance is not so good"
candidate_labels=["tech","sports","politics"]


result=classifier(text,candidate_labels=["tech","sports","politics"])

print(result)











